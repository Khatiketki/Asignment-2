# Asignment-2
Assignment 2: Naive CUDA GEMM Implementation 
Objective 
The goal of this assignment is to implement a naive Generalized Matrix Multiplication 
(GEMM) kernel using CUDA, without using high-level CUDA libraries such as cuBLAS or 
cuDNN. The implementation must rely solely on global memory and support optional 
transposition of input matrices. 
Implementation Overview 
A CUDA kernel was implemented to compute: 
ï¿½
ï¿½ â†ğ›¼â‹…ğ‘œğ‘(ğ´)â‹…ğ‘œğ‘(ğµ)+ğ›½â‹…ğ¶ 
where: 
â€¢ ğ´âˆˆâ„ğ‘šÃ—ğ‘˜ 
â€¢ ğµâˆˆâ„ğ‘˜Ã—ğ‘› 
â€¢ ğ¶âˆˆâ„ğ‘šÃ—ğ‘› 
â€¢ ğ‘œğ‘(ğ´)and ğ‘œğ‘(ğµ)may be either the original matrix or its transpose 
â€¢ ğ›¼and ğ›½are scalar coefficients 
Each CUDA thread computes one element of the output matrix C. The kernel uses a 2D grid 
of 2D thread blocks, and all memory accesses are performed using global memory only, 
as required. 
No optimizations such as shared memory, tiling, or loop unrolling were applied. 
Supported GEMM Variants 
The kernel supports all required combinations: 
â€¢ ğ¶â†ğ›¼ğ´ğµ+ğ›½ğ¶ 
â€¢ ğ¶â†ğ›¼ğ´ğ‘‡ğµ+ğ›½ğ¶ 
â€¢ ğ¶â†ğ›¼ğ´ğµğ‘‡+ğ›½ğ¶ 
â€¢ ğ¶â†ğ›¼ğ´ğ‘‡ğµğ‘‡+ğ›½ğ¶ 
The output matrix C is updated in place. 
Correctness Verification 
A CPU reference implementation of GEMM was used to verify correctness. The GPU results 
are compared element-wise against the CPU output, and mismatches are reported. 
Compilation 
The code was compiled using the NVIDIA CUDA compiler: 
nvcc -O2 -std=c++17 gemm_naive.cu -o gemm 
Execution Environment 
The local development machine is equipped with an IntelÂ® Arcâ„¢ 130V GPU, which is not 
CUDA-capable. CUDA requires an NVIDIA GPU and NVIDIA driver to execute kernels. 
At runtime, the program checks for a CUDA-capable device using cudaGetDeviceCount(). 
Since no NVIDIA GPU is present, the program exits gracefully with an informative message 
indicating that no CUDA device is available. This behavior is expected and confirms correct 
runtime error handling. 
The implementation is expected to run correctly on any system with a CUDA-capable 
NVIDIA GPU. 
Conclusion 
This assignment successfully implements a naive CUDA GEMM kernel that: 
â€¢ Uses only global memory 
â€¢ Supports optional transposition of input matrices 
â€¢ Updates the output matrix in place 
â€¢ Includes proper error handling and correctness verification 
The code adheres strictly to the assignment requirements and demonstrates correct use of 
CUDAâ€™s programming model. 
