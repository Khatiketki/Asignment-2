Name-Ketki Khati 
NU ID-002436886 
Assignment 2: Naive CUDA GEMM using Modal 
Objective 
The goal of this assignment is to implement and evaluate a naive matrix multiplication 
(GEMM) CUDA kernel and execute it on a GPU environment using Modal, since a local 
NVIDIA GPU was not available. The implementation computes: 
ï¿½
ï¿½ =ğ´Ã—ğµ 
where: 
â€¢ ğ´âˆˆâ„ğ‘€Ã—ğ¾ 
â€¢ ğµâˆˆâ„ğ¾Ã—ğ‘ 
â€¢ ğ¶âˆˆâ„ğ‘€Ã—ğ‘ 
System & Environment 
Local Machine 
â€¢ OS: Windows 
â€¢ GPU: IntelÂ® Arcâ„¢ 130V (non-CUDA, not supported by NVIDIA CUDA) 
â€¢ CUDA execution locally was not possible 
Remote GPU via Modal 
â€¢ Platform: Modal 
â€¢ CUDA Version: 12.2 
â€¢ GPUs used: 
o NVIDIA A10 
o NVIDIA L4 
â€¢ Compiler: nvcc 
â€¢ Execution: Remote GPU workers provisioned automatically by Modal 
Implementation Details 
CUDA Kernel 
A naive GEMM CUDA kernel was implemented where: 
â€¢ Each thread computes one element of matrix C 
â€¢ No shared memory optimizations were used 
â€¢ Kernel configuration: 
o 2D grid 
o 2D thread blocks 
Correctness Verification 
â€¢ GPU result is compared with a CPU reference implementation 
â€¢ Maximum absolute error is reported 
Execution via Modal 
Because no CUDA-capable NVIDIA GPU was available locally, the code was executed 
remotely using Modal. 
Workflow 
1. CUDA source file gemm_naive.cu was mounted to the Modal container 
2. CUDA container image (nvidia/cuda:12.2.0-devel-ubuntu22.04) was used 
3. Compilation performed inside Modal using nvcc 
4. Execution performed on Modal GPU workers 
5. Performance and correctness metrics recorded 
Command used: 
python -m modal run run_gemm_modal.py 
Experimental Results 
Matrix Size 
â€¢ ğ‘€=512 
â€¢ ğ‘=512 
â€¢ ğ¾=512 
Run 1 
â€¢ GPU: NVIDIA A10 
â€¢ Kernel Time: 0.157696 ms 
â€¢ Throughput: 1702.23 GFLOP/s 
â€¢ Max Absolute Error: 9.53674 Ã— 10â»â¶ 
Run 2 
â€¢ GPU: NVIDIA L4 
â€¢ Kernel Time: 0.164864 ms 
â€¢ Throughput: 1628.22 GFLOP/s 
â€¢ Max Absolute Error: 9.53674 Ã— 10â»â¶ 
Discussion 
â€¢ Both GPU executions produce numerically correct results, with very small 
f
loating-point error consistent with FP32 arithmetic. 
â€¢ Performance differs slightly due to: 
o Different GPU architectures (A10 vs L4) 
o Different clock speeds and memory characteristics 
â€¢ Despite being a naive kernel, the achieved throughput exceeds 1.6 TFLOP/s, 
demonstrating the effectiveness of GPU parallelism. 
â€¢ No shared memory or tiling optimizations were used; therefore, significant 
performance improvements are possible with optimized kernels. 
Challenges Faced 
â€¢ CUDA could not be executed locally due to absence of an NVIDIA GPU 
â€¢ Required setting up Modal authentication and remote execution 
â€¢ File mounting and container compilation issues were resolved during development 
Conclusion 
This assignment successfully demonstrates: 
â€¢ Implementation of a naive CUDA GEMM kernel 
â€¢ Remote GPU execution using Modal 
â€¢ Performance benchmarking on modern NVIDIA GPUs 
â€¢ Validation of correctness against CPU results 
The use of Modal enabled seamless access to CUDA-capable GPUs and allowed 
successful completion of the assignment without local NVIDIA hardware. 
